---
title: transformersã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¤ã„ã¦
tags:
  - Python
  - transformers
private: false
updated_at: '2024-02-22T21:43:14+09:00'
id: 00185f49f245c4d8df16
organization_url_name: null
slide: false
ignorePublish: false
---
## ç›®çš„  

transformersã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã™ã€‚

## å‹•æ©Ÿ

transformersã§ä½œæ¥­ã‚’ç¶šã‘ã¦ã„ã‚‹ã¨ãã€ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒ
å…¥ã‚‹ã¨å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ã«ãªã‚‹ã€‚Phi-2ã§ç´„5GBã®ä¿å­˜å®¹é‡ãŒå¿…è¦ã«ãªã‚Š
ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®åº¦ã«ãã‚Œãªã‚Šã®æ™‚é–“ãŒã‹ã‹ã‚Šç…©ã‚ã—ã„ã€‚(å…‰å›ç·šãŒå¼•ã‘ãªã„å®¶ğŸ¥º)

## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰

AutoTokenizerã‚„AutoModelForCausalLMã®save_pretrainedé–¢æ•°[[1]](#save_pretrainedé–¢æ•°ã«ã¤ã„ã¦)ã®save_directoryã¨ã„ã†å¼•æ•°ã«
ä¿å­˜å…ˆã‚’æŒ‡å®šã™ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šå ´æ‰€ã«ä¿å­˜ã§ãã‚‹ã€‚
ä¿å­˜ã—ãŸã‚ã¨ã¯from_pretrainedã®pretrained_model_name_or_pathå¼•æ•°ã«ä¿å­˜ã—ãŸå ´æ‰€ã‚’æŒ‡å®šã™ã‚Œã°
ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä½¿ç”¨ã§ãã‚‹ã€‚

```python

from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

def get_tokenizer_and_model(pre_trained_model_name, save_dir)
    if not Path(save_dir).exists():
        # download model
        self.tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name)
        self.model = AutoModelForCausalLM.from_pretrained(pre_trained_model_name)
        # save model
        self.tokenizer.save_pretrained(save_dir)
        self.model.save_pretrained(save_dir)

    else:
        # load model
        tokenizer = AutoTokenizer.from_pretrained(save_dir)
        model = AutoModelForCausalLM.from_pretrained(save_dir)

```  

## "the attention mask and the .."ã¨ã„ã†è­¦å‘Š

ã—ã‹ã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã•ã›ã‚ˆã†ã¨ã™ã‚‹ã¨æ¬¡ã®ã‚ˆã†ãªè­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œã‚‹ã€‚

```txt
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
```

ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ¸¡ã•ã‚Œã¦ã„ãªã„ãŸã‚ã«ç™ºç”Ÿã™ã‚‹ã€‚
é€šå¸¸ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³=EOSãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦æ‰±ã£ã¦ã—ã¾ã£ã¦å•é¡Œãªã„ã®ã§
æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã®ã‚ˆã†ã«pad_token_idã«tokenizer.eos_token_idã‚’è¨­å®šã™ã‚‹ã€‚[[2]](#the-attention-mask-and-the-ã¨ã„ã†è­¦å‘Šã«é–¢ã™ã‚‹è³ªå•)

```python
model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id)
```

ãã†ã™ã‚‹ã“ã¨ã§ä¸Šè¨˜ã®è­¦å‘Šã¯ç™ºç”Ÿã—ãªã„ã‚ˆã†ã«ã€‚

## ã¾ã¨ã‚

æ™‚ã€…å…¥ã‚‹å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ç…©ã‚ã•ã‚Œã‚‹ã“ã¨ãŒãªãå¿«é©ã«!!

## å‚è€ƒã‚µã‚¤ãƒˆ

### save_pretrainedé–¢æ•°ã«ã¤ã„ã¦

https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained

### "the attention mask and the .."ã¨ã„ã†è­¦å‘Šã«é–¢ã™ã‚‹è³ªå•

https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id/71397707#71397707
